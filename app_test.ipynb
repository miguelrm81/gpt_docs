{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    try:\n",
    "        path = f\"{os.getcwd()}\\\\uploads\"\n",
    "        for file in os.listdir(path):\n",
    "            python_file = file\n",
    "\n",
    "        file_path = os.path.join(path, python_file)        \n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            print(file_content)\n",
    "        answer = {'file': file_content}\n",
    "    except FileNotFoundError:\n",
    "        answer = { 'error' : f\"El archivo '{file_path}' no fue encontrado.\"}\n",
    "    except Exception as e:\n",
    "        answer = { 'error':  f\"Error al leer el archivo: {e}\"}\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "from flask import Flask, request\n",
      "import sqlite3\n",
      "import pickle\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "def clean_data(data):\n",
      "    columns = data.columns\n",
      "    if \"Unnamed: 0\" in columns:\n",
      "        data.drop(columns='Unnamed: 0', inplace=True)\n",
      "\n",
      "    index = data[data[\"newpaper\"].str.contains(r\".*[a-zA-Z].*\")].index  \n",
      "    data.loc[index,\"newpaper\"] = data.loc[index,\"newpaper\"].str.replace(\"s\",\"\")\n",
      "    data['newpaper'] = data['newpaper'].astype(float)\n",
      "    data.rename(columns={\"newpaper\" : \"newspaper\"}, inplace=True)\n",
      "\n",
      "    return data\n",
      "\n",
      "def connect_db():\n",
      "    conn = sqlite3.connect('advertising_new.db')\n",
      "\n",
      "    return conn\n",
      "\n",
      "def close_db(conn):\n",
      "    conn.close()\n",
      "\n",
      "def create_db(conn):\n",
      "    cursor = conn.cursor()\n",
      "\n",
      "    create_table = \\\n",
      "    \"\"\"\n",
      "        CREATE TABLE IF NOT EXISTS advertising (\n",
      "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "            TV FLOAT(32),\n",
      "            radio FLOAT(32),\n",
      "            newspaper FLOAT(32),\n",
      "            sales FLOAT(32)\n",
      "        )\n",
      "    \"\"\"\n",
      "    cursor.execute(create_table)\n",
      "\n",
      "\n",
      "def add_data(conn):\n",
      "\n",
      "    data = pd.read_csv('data/Advertising.csv')\n",
      "    data = clean_data(data)\n",
      "\n",
      "    cursor = conn.cursor()\n",
      "\n",
      "    query =\\\n",
      "    \"\"\"\n",
      "        SELECT * FROM advertising\n",
      "    \"\"\"\n",
      "    comp = cursor.execute(query)\n",
      "    results = comp.fetchall()\n",
      "\n",
      "    print(results)\n",
      "    if not results:\n",
      "        data.to_sql(name=\"advertising\", con=conn, if_exists=\"replace\", index=False)\n",
      "\n",
      "\n",
      "def ingest_new_data(conn, data):\n",
      "\n",
      "    cursor = conn.cursor()\n",
      "\n",
      "    try:\n",
      "        for reg in data:\n",
      "            tv = float(reg[0])\n",
      "            radio = float(reg[1])\n",
      "            newspaper = float(reg[2])\n",
      "            sales = float(reg[3])\n",
      "\n",
      "            query_2 = \\\n",
      "            \"\"\"\n",
      "                INSERT INTO advertising (TV, radio, newspaper, sales) VALUES (?, ?, ?, ?)\n",
      "\n",
      "            \"\"\"\n",
      "            cursor.execute(query_2, (tv, radio, newspaper, sales))\n",
      "            conn.commit()\n",
      "        \n",
      "        results = {'message': 'Datos ingresados correctamente'}\n",
      "\n",
      "    except Exception as e:\n",
      "        results = {\"message\": f'Error entering data : {str(e)}'}\n",
      "    \n",
      "    return results\n",
      "\n",
      "\n",
      "os.chdir(os.path.dirname(__file__))\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/', methods=['GET'])\n",
      "def welcome():\n",
      "    conn = connect_db()\n",
      "    create_db(conn)\n",
      "    add_data(conn)\n",
      "    close_db(conn)\n",
      "    return \"Welcome to mi API conected to predict model\"\n",
      "\n",
      "@app.route('/predict', methods=['GET'])\n",
      "def predict():\n",
      "    model = pickle.load(open('data/advertising_model','rb'))\n",
      "    conn = connect_db()\n",
      "    cursor = conn.cursor()\n",
      "    try:\n",
      "\n",
      "        if request.json:\n",
      "            results = {\"TV\": [], \"radio\": [], \"newspaper\": []}\n",
      "            data = request.json\n",
      "            input = data[\"data\"]\n",
      "            print(input)\n",
      "            for reg in input:\n",
      "                results[\"TV\"].append(float(reg[0]))\n",
      "                results[\"radio\"].append(float(reg[1]))\n",
      "                results[\"newspaper\"].append(float(reg[2]))\n",
      "                columns = results.keys()           \n",
      "        else:\n",
      "            query =\\\n",
      "            \"\"\"\n",
      "                SELECT TV, radio, newspaper FROM advertising\n",
      "            \"\"\"\n",
      "            data = cursor.execute(query)\n",
      "            results = data.fetchall()\n",
      "            columns = [descripcion[0] for descripcion in cursor.description]\n",
      "\n",
      "        print(results,columns)\n",
      "        X_test = pd.DataFrame(results, columns=columns)\n",
      "\n",
      "        prediction = model.predict(X_test)\n",
      "\n",
      "\n",
      "        msg = {'prediction': f'The prediction of sales investing that amount of money in TV, radio and newspaper is: {str(round(prediction[0],2))} k â‚¬'}\n",
      "\n",
      "    except Exception as e:\n",
      "        msg = {\"message\": f'Error in predict data: {str(e)}'}\n",
      "\n",
      "    close_db(conn)\n",
      "    return msg\n",
      "\n",
      "@app.route('/ingest', methods=[\"POST\"])\n",
      "def ingest_data():\n",
      "    conn = connect_db()\n",
      "    data = request.json\n",
      "    input = data[\"data\"]\n",
      "\n",
      "    comp = ingest_new_data(conn, input)\n",
      "\n",
      "    close_db(conn)\n",
      "    print(comp)\n",
      "    return comp\n",
      "\n",
      "@app.route('/retrain', methods=['POST'])\n",
      "def retrain():\n",
      "    conn = connect_db()\n",
      "    cursor = conn.cursor()\n",
      "    try:\n",
      "        model = pickle.load(open('data/advertising_model','rb'))\n",
      "\n",
      "        query =\\\n",
      "        \"\"\"\n",
      "            SELECT TV, radio, newspaper, sales FROM advertising\n",
      "        \"\"\"\n",
      "        data = cursor.execute(query)\n",
      "        results = data.fetchall()\n",
      "\n",
      "        columns = [descripcion[0] for descripcion in cursor.description]\n",
      "\n",
      "        data = pd.DataFrame(results, columns=columns)\n",
      "\n",
      "        X_train = data[[\"TV\", \"radio\", \"newspaper\"]]\n",
      "        y_train = data[\"sales\"]\n",
      "\n",
      "        model.fit(X_train, y_train)\n",
      "\n",
      "        with open('data/advertising_model', 'wb') as archivo:\n",
      "            pickle.dump(model, archivo)\n",
      "\n",
      "        msg = {'message': 'Modelo reentrenado correctamente.'}\n",
      "        \n",
      "    except Exception as e:\n",
      "        msg = {\"message\": f'Error in retrain model: {str(e)}'}\n",
      "    \n",
      "    close_db(conn)\n",
      "    return msg\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(host='0.0.0.0', port=5000, debug=True)\n"
     ]
    }
   ],
   "source": [
    "read_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt_response():\n",
    "\n",
    "    error = None\n",
    "    answer = {}\n",
    "    #question = request.json()\n",
    "    question_test = \"Cuentame que endpoints tiene el archivo\"\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "\n",
    "    qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "    qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
    "\n",
    "    try:\n",
    "        path = f\"{os.getcwd()}\\\\uploads\"\n",
    "        for py_file in os.listdir(path):\n",
    "            python_file = py_file\n",
    "\n",
    "        file_path = os.path.join(path, python_file)        \n",
    "        with open(file_path, 'r') as file_p:\n",
    "            file = file_p.read()\n",
    "\n",
    "        response = qa_document_chain.run(\n",
    "            input_document=file,\n",
    "            question=question_test,\n",
    "        )\n",
    "        time = datetime.now()\n",
    "        answer = {\n",
    "            'question': question_test,\n",
    "            'response': response,\n",
    "            'time': time.isoformat()\n",
    "        }\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        error = f\"El archivo '{file_path}' no fue encontrado.\"\n",
    "    except Exception as e:\n",
    "        error = f\"Error al leer el archivo: {e}\"\n",
    "\n",
    "    if error:\n",
    "        answer = {'error': error}\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Cuentame que endpoints tiene el archivo',\n",
       " 'response': 'El archivo tiene los siguientes endpoints:\\n\\n1. `/` : Este endpoint se utiliza para dar la bienvenida a la API y establecer la conexión con la base de datos.\\n\\n2. `/predict` : Este endpoint se utiliza para realizar predicciones utilizando un modelo de machine learning. Se puede enviar una solicitud GET para obtener las predicciones o se puede enviar una solicitud POST con datos de entrada en formato JSON para obtener predicciones personalizadas.\\n\\n3. `/ingest` : Este endpoint se utiliza para ingresar nuevos datos a la base de datos. Se debe enviar una solicitud POST con datos de entrada en formato JSON.\\n\\n4. `/retrain` : Este endpoint se utiliza para volver a entrenar el modelo de machine learning. Se debe enviar una solicitud POST.',\n",
       " 'time': '2023-12-14T17:00:16.712806'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-12-14T17:00:16.717630'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = datetime.now()\n",
    "time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
